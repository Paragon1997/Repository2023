{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkwant\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kwant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install qutip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge kwant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c pytorch pytorch-cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(60.).reshape(3, 4, 5)\n",
    "b = torch.arange(24.).reshape(4, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4400., 4730.],\n",
       "        [4532., 4874.],\n",
       "        [4664., 5018.],\n",
       "        [4796., 5162.],\n",
       "        [4928., 5306.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensordot(a, b, dims=([1, 0], [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 16\n",
    "r = torch.rand((2,n_particles)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 424),\n",
       "             ('active.all.current', 25),\n",
       "             ('active.all.freed', 399),\n",
       "             ('active.all.peak', 28),\n",
       "             ('active.large_pool.allocated', 58),\n",
       "             ('active.large_pool.current', 0),\n",
       "             ('active.large_pool.freed', 58),\n",
       "             ('active.large_pool.peak', 2),\n",
       "             ('active.small_pool.allocated', 366),\n",
       "             ('active.small_pool.current', 25),\n",
       "             ('active.small_pool.freed', 341),\n",
       "             ('active.small_pool.peak', 28),\n",
       "             ('active_bytes.all.allocated', 5193980928),\n",
       "             ('active_bytes.all.current', 590336),\n",
       "             ('active_bytes.all.freed', 5193390592),\n",
       "             ('active_bytes.all.peak', 2684945408),\n",
       "             ('active_bytes.large_pool.allocated', 5179965440),\n",
       "             ('active_bytes.large_pool.current', 0),\n",
       "             ('active_bytes.large_pool.freed', 5179965440),\n",
       "             ('active_bytes.large_pool.peak', 2684354560),\n",
       "             ('active_bytes.small_pool.allocated', 14015488),\n",
       "             ('active_bytes.small_pool.current', 590336),\n",
       "             ('active_bytes.small_pool.freed', 13425152),\n",
       "             ('active_bytes.small_pool.peak', 1246208),\n",
       "             ('allocated_bytes.all.allocated', 5193980928),\n",
       "             ('allocated_bytes.all.current', 590336),\n",
       "             ('allocated_bytes.all.freed', 5193390592),\n",
       "             ('allocated_bytes.all.peak', 2684945408),\n",
       "             ('allocated_bytes.large_pool.allocated', 5179965440),\n",
       "             ('allocated_bytes.large_pool.current', 0),\n",
       "             ('allocated_bytes.large_pool.freed', 5179965440),\n",
       "             ('allocated_bytes.large_pool.peak', 2684354560),\n",
       "             ('allocated_bytes.small_pool.allocated', 14015488),\n",
       "             ('allocated_bytes.small_pool.current', 590336),\n",
       "             ('allocated_bytes.small_pool.freed', 13425152),\n",
       "             ('allocated_bytes.small_pool.peak', 1246208),\n",
       "             ('allocation.all.allocated', 424),\n",
       "             ('allocation.all.current', 25),\n",
       "             ('allocation.all.freed', 399),\n",
       "             ('allocation.all.peak', 28),\n",
       "             ('allocation.large_pool.allocated', 58),\n",
       "             ('allocation.large_pool.current', 0),\n",
       "             ('allocation.large_pool.freed', 58),\n",
       "             ('allocation.large_pool.peak', 2),\n",
       "             ('allocation.small_pool.allocated', 366),\n",
       "             ('allocation.small_pool.current', 25),\n",
       "             ('allocation.small_pool.freed', 341),\n",
       "             ('allocation.small_pool.peak', 28),\n",
       "             ('inactive_split.all.allocated', 113),\n",
       "             ('inactive_split.all.current', 3),\n",
       "             ('inactive_split.all.freed', 110),\n",
       "             ('inactive_split.all.peak', 5),\n",
       "             ('inactive_split.large_pool.allocated', 28),\n",
       "             ('inactive_split.large_pool.current', 0),\n",
       "             ('inactive_split.large_pool.freed', 28),\n",
       "             ('inactive_split.large_pool.peak', 2),\n",
       "             ('inactive_split.small_pool.allocated', 85),\n",
       "             ('inactive_split.small_pool.current', 3),\n",
       "             ('inactive_split.small_pool.freed', 82),\n",
       "             ('inactive_split.small_pool.peak', 4),\n",
       "             ('inactive_split_bytes.all.allocated', 309123072),\n",
       "             ('inactive_split_bytes.all.current', 1506816),\n",
       "             ('inactive_split_bytes.all.freed', 307616256),\n",
       "             ('inactive_split_bytes.all.peak', 20971008),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 293601280),\n",
       "             ('inactive_split_bytes.large_pool.current', 0),\n",
       "             ('inactive_split_bytes.large_pool.freed', 293601280),\n",
       "             ('inactive_split_bytes.large_pool.peak', 18874368),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 15521792),\n",
       "             ('inactive_split_bytes.small_pool.current', 1506816),\n",
       "             ('inactive_split_bytes.small_pool.freed', 14014976),\n",
       "             ('inactive_split_bytes.small_pool.peak', 2096640),\n",
       "             ('max_split_size', -1),\n",
       "             ('num_alloc_retries', 1),\n",
       "             ('num_ooms', 1),\n",
       "             ('oversize_allocations.allocated', 0),\n",
       "             ('oversize_allocations.current', 0),\n",
       "             ('oversize_allocations.freed', 0),\n",
       "             ('oversize_allocations.peak', 0),\n",
       "             ('oversize_segments.allocated', 0),\n",
       "             ('oversize_segments.current', 0),\n",
       "             ('oversize_segments.freed', 0),\n",
       "             ('oversize_segments.peak', 0),\n",
       "             ('requested_bytes.all.allocated', 5193880634),\n",
       "             ('requested_bytes.all.current', 589856),\n",
       "             ('requested_bytes.all.freed', 5193290778),\n",
       "             ('requested_bytes.all.peak', 2684944448),\n",
       "             ('requested_bytes.large_pool.allocated', 5179965440),\n",
       "             ('requested_bytes.large_pool.current', 0),\n",
       "             ('requested_bytes.large_pool.freed', 5179965440),\n",
       "             ('requested_bytes.large_pool.peak', 2684354560),\n",
       "             ('requested_bytes.small_pool.allocated', 13915194),\n",
       "             ('requested_bytes.small_pool.current', 589856),\n",
       "             ('requested_bytes.small_pool.freed', 13325338),\n",
       "             ('requested_bytes.small_pool.peak', 1245248),\n",
       "             ('reserved_bytes.all.allocated', 2875195392),\n",
       "             ('reserved_bytes.all.current', 2149580800),\n",
       "             ('reserved_bytes.all.freed', 725614592),\n",
       "             ('reserved_bytes.all.peak', 2875195392),\n",
       "             ('reserved_bytes.large_pool.allocated', 2873098240),\n",
       "             ('reserved_bytes.large_pool.current', 2147483648),\n",
       "             ('reserved_bytes.large_pool.freed', 725614592),\n",
       "             ('reserved_bytes.large_pool.peak', 2873098240),\n",
       "             ('reserved_bytes.small_pool.allocated', 2097152),\n",
       "             ('reserved_bytes.small_pool.current', 2097152),\n",
       "             ('reserved_bytes.small_pool.freed', 0),\n",
       "             ('reserved_bytes.small_pool.peak', 2097152),\n",
       "             ('segment.all.allocated', 6),\n",
       "             ('segment.all.current', 2),\n",
       "             ('segment.all.freed', 4),\n",
       "             ('segment.all.peak', 6),\n",
       "             ('segment.large_pool.allocated', 5),\n",
       "             ('segment.large_pool.current', 1),\n",
       "             ('segment.large_pool.freed', 4),\n",
       "             ('segment.large_pool.peak', 5),\n",
       "             ('segment.small_pool.allocated', 1),\n",
       "             ('segment.small_pool.current', 1),\n",
       "             ('segment.small_pool.freed', 0),\n",
       "             ('segment.small_pool.peak', 1)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'max_split_size_mb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmax_split_size_mb \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'max_split_size_mb'"
     ]
    }
   ],
   "source": [
    "torch.cuda.max_split_size_mb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jordan_wigner_transform(j, lattice_length,device):\n",
    "    \"\"\"Jordan_wigner_transform(j, lattice_length). \n",
    "Defines the Jordan Wigner transformation for a 1D lattice.\"\"\"\n",
    "    operators = torch.tensor([[1,0],[0,-1]],device=device)\n",
    "    for _ in range(j-1): operators = torch.kron(operators, torch.tensor([[1,0],[0,-1]],device=device))\n",
    "    if j == 0: operators = torch.tensor([[0,0],[1,0]],device=device)\n",
    "    else: operators = torch.kron(operators, torch.tensor([[0,0],[1,0]],device=device))\n",
    "    for _ in range(lattice_length - j - 1): operators = torch.kron(operators, torch.tensor([[1,0],[0,1]],device=device))\n",
    "    return operators\n",
    "\n",
    "def Lorentzian(omega, Gamma, poles,Ed=-3/2,Sigma=3/2):\n",
    "    \"\"\"Lorentzian(omega, Gamma, poles,Ed=-3/2,Sigma=3/2). \n",
    "Defines the non-interacting DOS (rho0) and selects random sites based on the number of sites in the 1D lattice model and the calculated distribution.\"\"\"\n",
    "    p = torch.rand(poles)\n",
    "    return -np.imag(1/(omega-Ed-Sigma+1j*Gamma))/np.pi, np.array([Gamma * np.tan(np.pi * (p[i] - 1 / 2))+Ed+Sigma for i in range(poles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0653, -0.0449,  0.2221,  0.0385,  0.1308,  0.8144], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poles=6\n",
    "U=3\n",
    "Ed=-3/2\n",
    "Sigma=3/2\n",
    "Gamma=0.3\n",
    "torch.tensor([Gamma * torch.tan(np.pi * (pi - 1 / 2))+Ed+Sigma for pi in torch.rand(poles)],device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9090, 0.4626, 0.1245, 0.2387, 0.7980, 0.2960])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poles=6\n",
    "c=[Jordan_wigner_transform(i, 2*poles,device) for i in range(2*poles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m poles\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m\n\u001b[1;32m----> 2\u001b[0m c\u001b[39m=\u001b[39m[Jordan_wigner_transform(i, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpoles)\u001b[39m.\u001b[39mto_sparse() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpoles)]\n",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m poles\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m\n\u001b[1;32m----> 2\u001b[0m c\u001b[39m=\u001b[39m[Jordan_wigner_transform(i, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mpoles)\u001b[39m.\u001b[39mto_sparse() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpoles)]\n",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m, in \u001b[0;36mJordan_wigner_transform\u001b[1;34m(j, lattice_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: operators \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]])\n\u001b[0;32m      7\u001b[0m \u001b[39melse\u001b[39;00m: operators \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mkron(operators, torch\u001b[39m.\u001b[39mtensor([[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]]))\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lattice_length \u001b[39m-\u001b[39m j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m): operators \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mkron(operators, torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m]]))\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m operators\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "poles=8\n",
    "c=[Jordan_wigner_transform(i, 2*poles) for i in range(2*poles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(indices=tensor([[2048, 2049, 2050,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 2045, 2046, 2047]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[1024, 1025, 1026,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 3069, 3070, 3071]]),\n",
       "        values=tensor([ 1,  1,  1,  ..., -1, -1, -1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[ 512,  513,  514,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 3581, 3582, 3583]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[ 256,  257,  258,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 3837, 3838, 3839]]),\n",
       "        values=tensor([ 1,  1,  1,  ..., -1, -1, -1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[ 128,  129,  130,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 3965, 3966, 3967]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[  64,   65,   66,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 4029, 4030, 4031]]),\n",
       "        values=tensor([ 1,  1,  1,  ..., -1, -1, -1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[  32,   33,   34,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 4061, 4062, 4063]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[  16,   17,   18,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 4077, 4078, 4079]]),\n",
       "        values=tensor([ 1,  1,  1,  ..., -1, -1, -1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[   8,    9,   10,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 4085, 4086, 4087]]),\n",
       "        values=tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[   4,    5,    6,  ..., 4093, 4094, 4095],\n",
       "                        [   0,    1,    2,  ..., 4089, 4090, 4091]]),\n",
       "        values=tensor([ 1,  1,  1,  ..., -1, -1, -1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[   2,    3,    6,  ..., 4091, 4094, 4095],\n",
       "                        [   0,    1,    4,  ..., 4089, 4092, 4093]]),\n",
       "        values=tensor([ 1,  1, -1,  ..., -1,  1,  1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo),\n",
       " tensor(indices=tensor([[   1,    3,    5,  ..., 4091, 4093, 4095],\n",
       "                        [   0,    2,    4,  ..., 4090, 4092, 4094]]),\n",
       "        values=tensor([ 1, -1, -1,  ...,  1,  1, -1]),\n",
       "        size=(4096, 4096), nnz=2048, layout=torch.sparse_coo)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "crow_indices = [0, 1, 2]\n",
    "col_indices = [0,1]\n",
    "values = [1,-1]\n",
    "csr=torch.sparse_csr_tensor(torch.tensor(crow_indices, dtype=torch.int64),torch.tensor(col_indices, dtype=torch.int64),torch.tensor(values), dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, -1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,0],[0,-1]])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  0,  0,  0],\n",
       "        [ 0, -1,  0,  0],\n",
       "        [ 0,  0, -1,  0],\n",
       "        [ 0,  0,  0,  1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmaz=torch.tensor([[1,0],[0,-1]])\n",
    "torch.kron(sigmaz, sigmaz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "(Batch element 0) last value of crow_indices should be equal to the length of col_indices.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49msparse_csr_tensor(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64),torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64),torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: (Batch element 0) last value of crow_indices should be equal to the length of col_indices."
     ]
    }
   ],
   "source": [
    "torch.sparse_csr_tensor(torch.tensor([0,2,2], dtype=torch.int64),torch.tensor([0,1], dtype=torch.int64),torch.tensor([1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.,  9.],\n",
       "          [10., 11., 12., 13., 14.],\n",
       "          [15., 16., 17., 18., 19.]],\n",
       " \n",
       "         [[20., 21., 22., 23., 24.],\n",
       "          [25., 26., 27., 28., 29.],\n",
       "          [30., 31., 32., 33., 34.],\n",
       "          [35., 36., 37., 38., 39.]],\n",
       " \n",
       "         [[40., 41., 42., 43., 44.],\n",
       "          [45., 46., 47., 48., 49.],\n",
       "          [50., 51., 52., 53., 54.],\n",
       "          [55., 56., 57., 58., 59.]]]),\n",
       " tensor([[[ 0.,  1.],\n",
       "          [ 2.,  3.],\n",
       "          [ 4.,  5.]],\n",
       " \n",
       "         [[ 6.,  7.],\n",
       "          [ 8.,  9.],\n",
       "          [10., 11.]],\n",
       " \n",
       "         [[12., 13.],\n",
       "          [14., 15.],\n",
       "          [16., 17.]],\n",
       " \n",
       "         [[18., 19.],\n",
       "          [20., 21.],\n",
       "          [22., 23.]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
